# CFG-Parallel ç²¾åº¦é—®é¢˜åˆ†æ

## é—®é¢˜æè¿°

åœ¨æµ‹è¯• `riverclouds/qwen_image_random` æ¨¡å‹æ—¶å‘ç°ï¼š
- **num_inference_steps=4**: æ²¡æœ‰è¯¯å·®ï¼Œpixel å·®å¼‚ä¸ºé›¶ âœ…
- **num_inference_steps=8**: 30% çš„åƒç´ ä¸ä¸€è‡´ âŒ

## æ ¹æœ¬åŸå› åˆ†æ

### 1. Scheduler å†…éƒ¨çŠ¶æ€ä¸åŒæ­¥

FlowUniPCMultistepScheduler æ˜¯ä¸€ä¸ª **multistep solver**ï¼Œç»´æŠ¤ä»¥ä¸‹å†…éƒ¨çŠ¶æ€ï¼š

```python
# åœ¨ vllm_omni/diffusion/models/schedulers/scheduling_flow_unipc_multistep.py ç¬¬ 116-125 è¡Œ
self.model_outputs: list[torch.Tensor | None] = [None] * solver_order  # å†å²æ¨¡å‹è¾“å‡º
self.timestep_list: list[Any | None] = [None] * solver_order            # å†å²æ—¶é—´æ­¥
self.lower_order_nums = 0                                                # Warmup è®¡æ•°å™¨
self.last_sample: torch.Tensor | None = None                            # ä¸Šä¸€æ­¥çš„æ ·æœ¬
self._step_index: int | None = None                                      # å½“å‰æ­¥æ•°ç´¢å¼•
self.this_order: int = 1                                                 # å½“å‰ solver order
```

### 2. CFG-Parallel çš„æ‰§è¡Œæµç¨‹

åœ¨ `scheduler_step_maybe_with_cfg()` ä¸­ï¼ˆ`vllm_omni/diffusion/distributed/cfg_parallel.py` ç¬¬ 199-235 è¡Œï¼‰ï¼š

```python
if cfg_parallel_ready:
    cfg_rank = get_classifier_free_guidance_rank()
    
    # ğŸ”´ å…³é”®é—®é¢˜ï¼šåªæœ‰ rank 0 è®¡ç®— scheduler step
    if cfg_rank == 0:
        latents = self.scheduler_step(noise_pred, t, latents)
    
    # åªåŒæ­¥ latentsï¼Œscheduler å†…éƒ¨çŠ¶æ€æœªåŒæ­¥
    latents = latents.contiguous()
    cfg_group.broadcast(latents, src=0)
```

**æ‰§è¡Œå·®å¼‚ï¼š**

| æ“ä½œ | CFG-Parallel (cfg_parallel_size=2) | Sequential (cfg_parallel_size=1) |
|------|-------------------------------------|----------------------------------|
| Predict Noise | Rank 0: positive<br>Rank 1: negative | å•å¡ï¼šå…ˆ positiveï¼Œå negative |
| Scheduler Step | **ä»… Rank 0 æ‰§è¡Œ** | å•å¡ï¼šæ‰§è¡Œä¸€æ¬¡ |
| Scheduler çŠ¶æ€æ›´æ–° | **ä»… Rank 0 æ›´æ–°** | å•å¡ï¼šæ­£å¸¸æ›´æ–° |
| Latents åŒæ­¥ | Broadcast ä» rank 0 åˆ°æ‰€æœ‰ ranks | ä¸éœ€è¦ |

### 3. é—®é¢˜çš„å…³é”®

**Rank 1 çš„ scheduler å†…éƒ¨çŠ¶æ€ä»æœªè¢«æ›´æ–°ï¼**

è™½ç„¶ latents åœ¨æ¯ä¸€æ­¥åé€šè¿‡ `broadcast` åŒæ­¥ï¼Œä½† scheduler çš„å†…éƒ¨çŠ¶æ€ï¼ˆå†å²ä¿¡æ¯ï¼‰**æ²¡æœ‰åŒæ­¥**ã€‚è¿™å¯¼è‡´ï¼š

1. Rank 1 çš„ scheduler ä¿æŒåˆå§‹çŠ¶æ€
2. åœ¨ä¸‹ä¸€æ¬¡å¾ªç¯ä¸­ï¼Œè™½ç„¶ latents æ˜¯åŒæ­¥çš„ï¼Œä½† rank 1 çš„ scheduler ä»ç„¶è®¤ä¸ºè‡ªå·±å¤„äºåˆå§‹çŠ¶æ€
3. **å®é™…ä¸Šè¿™ä¸åº”è¯¥å½±å“ predict_noiseï¼Œå› ä¸º predict_noise ä¸ä¾èµ– scheduler çŠ¶æ€**

### 4. çœŸæ­£çš„é—®é¢˜æ¥æº

ç­‰ç­‰ï¼Œè®©æˆ‘é‡æ–°å®¡è§†...å¦‚æœ predict_noise ä¸ä¾èµ– scheduler çŠ¶æ€ï¼Œé‚£ä¸ºä»€ä¹ˆä¼šæœ‰å·®å¼‚ï¼Ÿ

**å¯èƒ½çš„åŸå› ï¼š**

#### å‡è®¾ A: æ•°å€¼ç²¾åº¦ç´¯ç§¯è¯¯å·®
- CFG parallel å’Œ sequential æ¨¡å¼ä¸‹çš„è®¡ç®—é¡ºåºä¸åŒ
- Floating point è¿ç®—çš„èˆå…¥è¯¯å·®å¯èƒ½ç´¯ç§¯
- ä½†è¿™ä¸åº”è¯¥å¯¼è‡´ 30% çš„åƒç´ å·®å¼‚...

#### å‡è®¾ B: Scheduler çš„ multistep å†å²å½±å“ä¸‹ä¸€æ­¥çš„è®¡ç®—
è®©æˆ‘æ£€æŸ¥ scheduler æ˜¯å¦åœ¨æŸå¤„è¢«å¼•ç”¨...

å®é™…ä¸Šï¼Œæˆ‘æ„è¯†åˆ°ä¸€ä¸ªå…³é”®ç‚¹ï¼šåœ¨æ¯æ¬¡å¾ªç¯å¼€å§‹æ—¶ï¼Œä¸¤ä¸ª ranks éƒ½ä¼šä½¿ç”¨ç›¸åŒçš„ `latents`ï¼ˆé€šè¿‡ä¸Šä¸€æ­¥çš„ broadcast åŒæ­¥ï¼‰ï¼Œä½†æ˜¯...

**å•Šå“ˆï¼æˆ‘æ‰¾åˆ°äº†ï¼**

é—®é¢˜å¯èƒ½åœ¨äº **scheduler çš„ multistep ç®—æ³•ä¾èµ–å†å²ä¿¡æ¯æ¥è®¡ç®—å½“å‰æ­¥**ã€‚

åœ¨ `scheduler.step()` ç¬¬ 670-674 è¡Œï¼š

```python
prev_sample = self.multistep_uni_p_bh_update(
    model_output=model_output,  # ğŸ”´ å½“å‰è¾“å‡º
    sample=sample,               # ğŸ”´ å½“å‰æ ·æœ¬
    order=self.this_order,       # ğŸ”´ ä¾èµ– this_orderï¼ˆåŸºäº lower_order_numsï¼‰
)
```

`multistep_uni_p_bh_update` ä½¿ç”¨ `self.model_outputs` å†å²æ¥è¿›è¡Œé«˜é˜¶é¢„æµ‹ã€‚ä½†è¿™ä¸ªå‡½æ•°åªåœ¨ rank 0 ä¸Šæ‰§è¡Œï¼Œrank 1 çš„ scheduler ä»æœªæ›´æ–°å…¶å†å²ã€‚

**ä½†å…³é”®æ˜¯ï¼šrank 1 åœ¨ä¸‹ä¸€æ­¥å¾ªç¯ä¸­ä¸ä¼šè°ƒç”¨ scheduler.step()ï¼Œå®ƒåªå‚ä¸ predict_noiseã€‚**

æ‰€ä»¥ç†è®ºä¸Šä¸åº”è¯¥æœ‰é—®é¢˜...é™¤é...

#### å‡è®¾ C: æŸäº›å…±äº«çš„å…¨å±€çŠ¶æ€æˆ– cache

è®©æˆ‘æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½• cache æˆ–å…¨å±€çŠ¶æ€åœ¨ä¸¤ä¸ªæ¨¡å¼ä¸‹è¡¨ç°ä¸åŒã€‚

## æ ¸å¿ƒé—®é¢˜

### Scheduler çŠ¶æ€åœ¨ Rank 1 æœªæ›´æ–°

åœ¨ `scheduler_step_maybe_with_cfg()` ä¸­ï¼š

```python
if cfg_parallel_ready:
    cfg_rank = get_classifier_free_guidance_rank()
    
    if cfg_rank == 0:
        latents = self.scheduler_step(noise_pred, t, latents)  # âœ… æ›´æ–° scheduler çŠ¶æ€
    # else: rank 1 ä»€ä¹ˆéƒ½ä¸åšï¼âŒ
    
    latents = latents.contiguous()
    cfg_group.broadcast(latents, src=0)  # åªåŒæ­¥ latentsï¼Œä¸åŒæ­¥ scheduler çŠ¶æ€
```

**ç»“æœï¼š**
- Rank 0 çš„ scheduler çŠ¶æ€æ­£å¸¸æ›´æ–°ï¼ˆmodel_outputs, timestep_list, last_sample, lower_order_nums, step_indexï¼‰
- Rank 1 çš„ scheduler çŠ¶æ€**ä¿æŒåˆå§‹å€¼**ï¼Œä»æœªæ›´æ–°

### ä¸ºä»€ä¹ˆè¿™ä¼šå¯¼è‡´é—®é¢˜ï¼Ÿ

è™½ç„¶ `predict_noise` ä¸ç›´æ¥ä¾èµ– scheduler çŠ¶æ€ï¼Œä½†**å¯èƒ½çš„åŸå› åŒ…æ‹¬ï¼š**

1. **Scheduler Multistep å†å²ä¾èµ–**ï¼š
   - FlowUniPCMultistepScheduler ä½¿ç”¨å†å² model_outputs æ¥è®¡ç®—é«˜é˜¶é¢„æµ‹
   - è™½ç„¶åªæœ‰ rank 0 æ‰§è¡Œ scheduler.step()ï¼Œä½†å¦‚æœæœ‰ä»»ä½•å…±äº«çŠ¶æ€æˆ–å…¨å±€å˜é‡...

2. **Cache æœºåˆ¶çš„çŠ¶æ€è¿½è¸ª**ï¼š
   - TeaCache ä½¿ç”¨ `do_true_cfg` å’Œ cfg_rank æ¥åŒºåˆ† positive/negative åˆ†æ”¯
   - å¦‚æœå¯ç”¨äº† cacheï¼Œå¯èƒ½ä¼šå› ä¸º scheduler çŠ¶æ€ä¸åŒè€Œäº§ç”Ÿä¸ä¸€è‡´

3. **æ•°å€¼ç²¾åº¦ç´¯ç§¯è¯¯å·®**ï¼š
   - CFG-parallel: positive å’Œ negative å¹¶è¡Œè®¡ç®—ï¼Œç„¶åç»„åˆ
   - Sequential: positive å…ˆè®¡ç®—ï¼Œç„¶å negativeï¼Œæœ€åç»„åˆ
   - æµ®ç‚¹è¿ç®—é¡ºåºä¸åŒå¯èƒ½å¯¼è‡´èˆå…¥è¯¯å·®ç´¯ç§¯

4. **Transformer å†…éƒ¨çŠ¶æ€**ï¼š
   - å¦‚æœ transformer æœ‰ä»»ä½•ä¾èµ–äº"å½“å‰æ˜¯ç¬¬å‡ æ­¥"çš„é€»è¾‘
   - ä½†è¿™ä¸å¤ªå¯èƒ½ï¼Œå› ä¸º transformer åº”è¯¥æ˜¯æ— çŠ¶æ€çš„

### ä¸ºä»€ä¹ˆ steps=4 æ— æŸï¼Œsteps=8 æœ‰æŸï¼Ÿ

Scheduler çš„ warmup é˜¶æ®µï¼š

```python
# scheduling_flow_unipc_multistep.py
if self.lower_order_nums < self.config.solver_order:
    self.lower_order_nums += 1
```

- **Steps 1-3**: `lower_order_nums < solver_order`ï¼ˆé€šå¸¸ solver_order=2ï¼‰ï¼Œä½¿ç”¨ä½é˜¶æ–¹æ³•
- **Steps 4+**: è¿›å…¥å®Œæ•´çš„ multistep æ¨¡å¼ï¼Œå¼€å§‹ä½¿ç”¨å†å² model_outputs

æ¨æµ‹ï¼š
- å½“ steps â‰¤ 4 æ—¶ï¼Œscheduler è¿˜åœ¨ warmupï¼Œä¸ä¾èµ–æˆ–å¾ˆå°‘ä¾èµ–å†å²
- å½“ steps > 4 æ—¶ï¼Œscheduler å¼€å§‹ä¸¥é‡ä¾èµ–å†å² model_outputsï¼Œè€Œ rank 1 çš„å†å²æ˜¯ç©ºçš„æˆ–è¿‡æ—¶çš„

## è¯Šæ–­æ­¥éª¤

### 1. è¿è¡Œè¯Šæ–­è„šæœ¬

```bash
python diagnose_cfg_parallel.py
```

è¿™ä¸ªè„šæœ¬ä¼šæµ‹è¯•ä¸åŒæ¨ç†æ­¥æ•°ï¼ˆ2, 4, 6, 8, 10, 12ï¼‰ä¸‹çš„å·®å¼‚ï¼Œå¸®åŠ©ç¡®è®¤é—®é¢˜ä½•æ—¶å¼€å§‹å‡ºç°ã€‚

### 2. æ£€æŸ¥æ˜¯å¦å¯ç”¨äº† cache

åœ¨ç”¨æˆ·çš„æµ‹è¯•ä¸­ï¼Œç¡®è®¤æ˜¯å¦ä¼ é€’äº† `cache_backend` å‚æ•°ã€‚å¦‚æœå¯ç”¨äº† cacheï¼Œè¿™å¯èƒ½å¯¼è‡´é¢å¤–çš„çŠ¶æ€ä¸ä¸€è‡´ã€‚

### 3. æ·»åŠ  scheduler çŠ¶æ€æ—¥å¿—

ä¿®æ”¹ `scheduler_step_maybe_with_cfg()` æ¥è®°å½• scheduler çš„å†…éƒ¨çŠ¶æ€ï¼Œå¯¹æ¯”ä¸¤ä¸ª ranks çš„çŠ¶æ€å·®å¼‚ã€‚

## å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ A: åŒæ­¥ Scheduler çŠ¶æ€åˆ°æ‰€æœ‰ Ranks

ä¿®æ”¹ `scheduler_step_maybe_with_cfg()` æ¥åŒæ­¥ scheduler çš„å†…éƒ¨çŠ¶æ€ï¼š

```python
def scheduler_step_maybe_with_cfg(
    self, noise_pred: torch.Tensor, t: torch.Tensor, latents: torch.Tensor, do_true_cfg: bool
) -> torch.Tensor:
    cfg_parallel_ready = do_true_cfg and get_classifier_free_guidance_world_size() > 1
    
    if cfg_parallel_ready:
        cfg_group = get_cfg_group()
        cfg_rank = get_classifier_free_guidance_rank()
        
        if cfg_rank == 0:
            latents = self.scheduler_step(noise_pred, t, latents)
        
        latents = latents.contiguous()
        cfg_group.broadcast(latents, src=0)
        
        # ğŸ”§ æ–°å¢ï¼šåŒæ­¥ scheduler çŠ¶æ€
        # éœ€è¦åŒæ­¥ï¼šmodel_outputs, timestep_list, last_sample, lower_order_nums, step_index, this_order
        if cfg_rank == 0:
            # Rank 0 å‡†å¤‡çŠ¶æ€æ•°æ®
            state_dict = {
                'lower_order_nums': self.scheduler.lower_order_nums,
                '_step_index': self.scheduler._step_index,
                'this_order': self.scheduler.this_order,
                # model_outputs å’Œ timestep_list éœ€è¦ç‰¹æ®Šå¤„ç†
            }
        else:
            state_dict = None
        
        # Broadcast çŠ¶æ€åˆ°æ‰€æœ‰ ranks
        # ... éœ€è¦å®ç°çŠ¶æ€åºåˆ—åŒ–å’Œååºåˆ—åŒ– ...
```

**é—®é¢˜**: è¿™ä¸ªæ–¹æ¡ˆå¤æ‚ï¼Œéœ€è¦åºåˆ—åŒ–/ååºåˆ—åŒ– scheduler çš„å†…éƒ¨çŠ¶æ€ã€‚

### æ–¹æ¡ˆ B: è®©æ‰€æœ‰ Ranks éƒ½æ‰§è¡Œ Scheduler Stepï¼ˆæ¨èï¼‰

æ›´ç®€å•çš„æ–¹æ¡ˆï¼šè®© rank 1 ä¹Ÿæ‰§è¡Œ scheduler.step()ï¼Œå³ä½¿å®ƒçš„ noise_pred ä¸ä¼šè¢«ä½¿ç”¨ã€‚

```python
def scheduler_step_maybe_with_cfg(
    self, noise_pred: torch.Tensor, t: torch.Tensor, latents: torch.Tensor, do_true_cfg: bool
) -> torch.Tensor:
    cfg_parallel_ready = do_true_cfg and get_classifier_free_guidance_world_size() > 1
    
    if cfg_parallel_ready:
        cfg_group = get_cfg_group()
        cfg_rank = get_classifier_free_guidance_rank()
        
        # ğŸ”§ ä¿®æ”¹ï¼šæ‰€æœ‰ ranks éƒ½æ‰§è¡Œ scheduler stepï¼ˆä¿æŒçŠ¶æ€ä¸€è‡´ï¼‰
        if cfg_rank == 0:
            latents = self.scheduler_step(noise_pred, t, latents)
        else:
            # Rank 1 ä¹Ÿæ‰§è¡Œ step æ¥æ›´æ–°å†…éƒ¨çŠ¶æ€ï¼Œä½†ä½¿ç”¨ rank 0 broadcast çš„ latents
            # æ³¨æ„ï¼šrank 1 çš„ noise_pred æ˜¯ Noneï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ª dummy å€¼æˆ–ä½¿ç”¨ latents
            dummy_noise = torch.zeros_like(latents)
            _ = self.scheduler_step(dummy_noise, t, latents)
        
        # åŒæ­¥æœ€ç»ˆçš„ latentsï¼ˆåªä½¿ç”¨ rank 0 çš„ç»“æœï¼‰
        latents = latents.contiguous()
        cfg_group.broadcast(latents, src=0)
```

**ä¼˜ç‚¹**: ç®€å•ï¼Œä¿æŒæ‰€æœ‰ ranks çš„ scheduler çŠ¶æ€ä¸€è‡´
**ç¼ºç‚¹**: rank 1 åšäº†é¢å¤–çš„è®¡ç®—ï¼ˆä½†å¾ˆå°ï¼‰

### æ–¹æ¡ˆ C: ç¦ç”¨ Multistep æ–¹æ³•

ä½¿ç”¨æ›´ç®€å•çš„ Euler schedulerï¼Œé¿å…ä¾èµ–å†å²çŠ¶æ€ï¼š

```python
# åœ¨ pipeline åˆå§‹åŒ–æ—¶
if od_config.parallel_config.cfg_parallel_size > 1:
    # ä½¿ç”¨ Euler scheduler è€Œä¸æ˜¯ UniPC multistep
    self.scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(...)
```

**ä¼˜ç‚¹**: é¿å…çŠ¶æ€åŒæ­¥é—®é¢˜
**ç¼ºç‚¹**: å¯èƒ½éœ€è¦æ›´å¤šçš„æ¨ç†æ­¥æ•°æ‰èƒ½è¾¾åˆ°ç›¸åŒè´¨é‡

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. è¿è¡Œ `diagnose_cfg_parallel.py` ç¡®è®¤é—®é¢˜æ¨¡å¼
2. æ£€æŸ¥æ˜¯å¦æœ‰é»˜è®¤å¯ç”¨çš„ cache
3. å®æ–½æ–¹æ¡ˆ Bï¼ˆæœ€ç®€å•æœ‰æ•ˆï¼‰
4. æ·»åŠ é›†æˆæµ‹è¯•éªŒè¯å¤šæ­¥æ¨ç†çš„æ— æŸæ€§

</thinking>

<function_calls>
<invoke name="SemanticSearch">
<parameter name="query">Does the transformer or attention mechanism have any internal state or use random numbers during inference?